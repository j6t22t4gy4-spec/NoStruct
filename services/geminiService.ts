
import { GoogleGenAI, Type, Modality } from "@google/genai";
import { AppMode, Message } from "../types";

// System Instruction focusing on Educational accuracy and Hallucination suppression
const SYSTEM_INSTRUCTION = `You are NoStruct, a world-class educational assistant. 
Your primary goal is to provide accurate, factual, and logical explanations.
CRITICAL: Avoid hallucinations at all costs. If you are unsure of a fact, state that you don't know or use the provided search tools.
Cite sources when using search grounding.
When thinking is enabled, provide deep step-by-step logical reasoning.
Maintain a professional, encouraging, and clear educational tone.
If the user asks for a visual representation, diagram, or image, and you are in a mode that supports it (like gemini-2.5-flash-image), generate and provide the image data inline.`;

// Use process.env.API_KEY directly as per guidelines
export const getAIClient = () => new GoogleGenAI({ apiKey: process.env.API_KEY });

export const generateEducationalResponse = async (
  mode: AppMode,
  prompt: string,
  history: Message[] = [],
  mediaParts: any[] = []
) => {
  const ai = getAIClient();
  let model = 'gemini-3-flash-preview'; 
  let config: any = { systemInstruction: SYSTEM_INSTRUCTION };

  // Mode-based Model and Config Selection
  switch (mode) {
    case AppMode.SEARCH:
      model = 'gemini-3-flash-preview';
      config.tools = [{ googleSearch: {} }];
      break;
    case AppMode.THINKING:
      model = 'gemini-3-pro-preview'; // Upgrade to Pro for better reasoning
      config.thinkingConfig = { thinkingBudget: 16000 };
      break;
    case AppMode.VISION:
      model = 'gemini-2.5-flash-image'; 
      break;
    case AppMode.FAST:
      model = 'gemini-flash-lite-latest';
      break;
    default:
      model = 'gemini-3-flash-preview';
  }

  // Format history for the API
  const contents = [
    ...history.slice(-10).map(m => ({ // Send last 10 messages for context
      role: m.role === 'user' ? 'user' : 'model',
      parts: m.mediaData && m.role === 'user' ? [
        { inlineData: { mimeType: m.mediaType, data: m.mediaData.split(',')[1] } },
        { text: m.content }
      ] : [{ text: m.content }]
    })),
    {
      role: 'user',
      parts: mediaParts.length > 0 ? [...mediaParts, { text: prompt }] : [{ text: prompt }]
    }
  ];

  const response = await ai.models.generateContent({
    model,
    contents,
    config
  });

  const groundingChunks = response.candidates?.[0]?.groundingMetadata?.groundingChunks || [];
  const urls = groundingChunks
    .map((chunk: any) => chunk.web?.uri)
    .filter((uri: string) => !!uri);

  let aiMedia: { data: string, mimeType: string } | undefined = undefined;
  let thinkingText = "";
  let finalResponseText = response.text || "";
  
  const parts = response.candidates?.[0]?.content?.parts || [];
  
  for (const part of parts) {
    // Extract thinking tokens (for Gemini 3 models)
    if ((part as any).thought) {
      thinkingText += (part as any).text;
    }
    // Extract inline image/media data generated by the model
    if (part.inlineData) {
      aiMedia = {
        data: `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`,
        mimeType: part.inlineData.mimeType
      };
    }
  }

  return {
    text: finalResponseText,
    urls,
    thinking: thinkingText || undefined,
    aiMedia
  };
};

export const transcribeAudio = async (base64Audio: string) => {
  const ai = getAIClient();
  const response = await ai.models.generateContent({
    model: 'gemini-3-flash-preview',
    contents: {
      parts: [
        { inlineData: { mimeType: 'audio/wav', data: base64Audio } },
        { text: '이 오디오를 정확하게 받아쓰기 하세요.' }
      ]
    }
  });
  return response.text;
};

// Live Session Utils
export const encodePCM = (bytes: Uint8Array) => {
  let binary = '';
  const len = bytes.byteLength;
  for (let i = 0; i < len; i++) {
    binary += String.fromCharCode(bytes[i]);
  }
  return btoa(binary);
};

export const decodePCM = (base64: string) => {
  const binaryString = atob(base64);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes;
};

export async function decodeAudioData(
  data: Uint8Array,
  ctx: AudioContext,
  sampleRate: number,
  numChannels: number,
): Promise<AudioBuffer> {
  const dataInt16 = new Int16Array(data.buffer);
  const frameCount = dataInt16.length / numChannels;
  const buffer = ctx.createBuffer(numChannels, frameCount, sampleRate);

  for (let channel = 0; channel < numChannels; channel++) {
    const channelData = buffer.getChannelData(channel);
    for (let i = 0; i < frameCount; i++) {
      channelData[i] = dataInt16[i * numChannels + channel] / 32768.0;
    }
  }
  return buffer;
}
